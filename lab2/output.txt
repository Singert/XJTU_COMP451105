子文件夹：./core
子文件夹：./core/dfa
文件：./core/dfa/dfa.go
内容：
package dfa

import (
	"fmt"
	"lab2/core/utils"
	"os"
)

func (d *DFA) buildAcceptMap() {
	d.acceptMap = make(map[string]bool)
	for _, s := range d.AcceptStates {
		d.acceptMap[s] = true
	}
}

func (d *DFA) ExportDFAtoDot(filename string) error {
	file, err := os.Create(filename)
	if err != nil {
		return err
	}
	defer file.Close()

	fmt.Fprintln(file, "digraph DFA {")
	fmt.Fprintln(file, "  rankdir=LR;")

	// 初始状态箭头（空节点到起始状态）
	fmt.Fprintf(file, `  "" -> %s;`+"\n", d.StartState)

	// 输出所有状态节点
	for _, s := range d.States {
		shape := "circle"
		if d.acceptMap[s] {
			shape = "doublecircle"
		}
		fmt.Fprintf(file, `  %s [shape=%s];`+"\n", s, shape)
	}

	// 输出转移边
	for from, transitions := range d.Transitions {
		for symbol, to := range transitions {
			fmt.Fprintf(file, `  %s -> %s [label="%s"];`+"\n", from, to, symbol)
		}
	}

	fmt.Fprintln(file, "}")
	return nil
}

func (d *DFA) MatchDFA(input string, verbose bool) (bool, []TransitionTrace) {
	currentState := d.StartState
	trace := []TransitionTrace{}
	for i, ch := range input {
		symbol := string(ch)
		if !utils.Contains(d.Alphabet, symbol) {
			if verbose {
				fmt.Printf("Step %d: %s --%s--> ❌invalid symbol\n", i+1, currentState, symbol)
			}
			return false, nil
		}
		nextState, ok := d.Transitions[currentState][symbol]
		if !ok {
			if verbose {
				fmt.Printf("Step %d: %s --%s--> ❌invalid transition\n", i+1, currentState, symbol)
			}
			return false, nil
		}

		trace = append(trace, TransitionTrace{
			From:   currentState,
			Symbol: symbol,
			To:     nextState,
		})

		if verbose {
			fmt.Printf("Step %d: %s --%s--> %s\n", i+1, currentState, symbol, nextState)
		}
		currentState = nextState
	}
	return d.acceptMap[currentState], trace
}

func (d *DFA) EnumValidStrings(maxLength int) []string {
	var validStrings []string
	var currentString string
	var currentState string

	// BFS queue
	queue := make([]struct {
		state  string
		string string
		length int
	}, 0)

	// Start with the initial state and an empty string
	queue = append(queue, struct {
		state  string
		string string
		length int
	}{d.StartState, "", 0})

	for len(queue) > 0 {
		item := queue[0]
		queue = queue[1:]

		currentState = item.state
		currentString = item.string

		if item.length > maxLength {
			continue
		}

		if d.acceptMap[currentState] && item.length <= maxLength {
			validStrings = append(validStrings, currentString)
		}

		for _, symbol := range d.Alphabet {
			nextState, ok := d.Transitions[currentState][symbol]
			if ok {
				queue = append(queue, struct {
					state  string
					string string
					length int
				}{nextState, currentString + symbol, item.length + 1})
			}
		}
	}
	return validStrings
}

func (d *DFA) CheckValidity(verbose bool, tokenType TokenType) bool {
	fmt.Printf("[Checking DFA Validity]: <%s>\n", tokenType)

	// 1️⃣ check start_state not nil
	if verbose {
		fmt.Print("checking start_state not nil: ")
	}
	if d.StartState == "" {
		if verbose {
			fmt.Print("❌ Start state is nil\n")
		}
		fmt.Println("[DFA Invalid]")
		return false
	} else if verbose {
		fmt.Println("PASS")
	}

	// 2️⃣ check start_state in states
	if verbose {
		fmt.Print("checking start_state in states: ")
	}
	if !utils.Contains(d.States, d.StartState) {
		if verbose {
			fmt.Printf("❌ Start state %s not in states\n", d.StartState)
		}
		fmt.Println("[DFA Invalid]")
		return false
	} else if verbose {
		fmt.Println("PASS")
	}

	// 3️⃣ check accept_states not nil
	if verbose {
		fmt.Print("checking accept_states not nil: ")
	}
	if len(d.AcceptStates) == 0 {
		if verbose {
			fmt.Print("❌ Accept states is nil\n")
		}
		fmt.Println("[DFA Invalid]")
		return false
	} else if verbose {
		fmt.Println("PASS")
	}

	// 4️⃣ check accept_states in states
	if verbose {
		fmt.Print("checking accept_states in states: ")
	}
	for _, s := range d.AcceptStates {
		if !utils.Contains(d.States, s) {
			if verbose {
				fmt.Printf("❌ Accept state %s not in states\n", s)
			}
			fmt.Println("[DFA Invalid]")
			return false
		}
	}
	if verbose {
		fmt.Println("PASS")
	}

	// 5️⃣ check transitions not nil
	if verbose {
		fmt.Print("checking transitions not nil: ")
	}
	if len(d.Transitions) == 0 {
		if verbose {
			fmt.Print("❌ Transitions is nil\n")
		}
		fmt.Println("[DFA Invalid]")
		return false
	} else if verbose {
		fmt.Println("PASS")
	}

	// 6️⃣ check all transitions refer to valid states and symbols
	if verbose {
		fmt.Print("checking transitions for valid states and symbols: ")
	}
	for from, trans := range d.Transitions {
		if !utils.Contains(d.States, from) {
			if verbose {
				fmt.Printf("❌ Transition state %s not in states\n", from)
			}
			fmt.Println("[DFA Invalid]")
			return false
		}
		for symbol, to := range trans {
			if !utils.Contains(d.Alphabet, symbol) {
				if verbose {
					fmt.Printf("❌ Transition symbol %s not in alphabet\n", symbol)
				}
				fmt.Println("[DFA Invalid]")
				return false
			}
			if !utils.Contains(d.States, to) {
				if verbose {
					fmt.Printf("❌ Transition destination %s not in states\n", to)
				}
				fmt.Println("[DFA Invalid]")
				return false
			}
		}
	}
	if verbose {
		fmt.Println("PASS")
	}
	for _, s := range d.States {
		for _, sym := range d.Alphabet {
			if _, ok := d.Transitions[s][sym]; !ok {
				if verbose {
					fmt.Printf("[CheckValidity] Warning: State '%s' missing transition on symbol '%s'\n", s, sym)
				}
			}
		}
	}
	fmt.Println("[DFA Valid]")
	return true
}

func (d *DFA) ExportToDot(filename string, trace []TransitionTrace) error {
	file, err := os.Create(filename)
	if err != nil {
		return err
	}
	defer file.Close()

	fmt.Fprintln(file, "digraph DFA {")
	fmt.Fprintln(file, "  rankdir=LR;")
	fmt.Fprintln(file, `  "" -> `+d.StartState+`;`)

	// accept states double circle
	for _, s := range d.States {
		shape := "circle"
		if d.acceptMap[s] {
			shape = "doublecircle"
		}
		fmt.Fprintf(file, "  %s [shape=%s];\n", s, shape)
	}

	// mark all matched paths with highlight color
	edgeMark := make(map[string]bool)
	for _, t := range trace {
		key := fmt.Sprintf("%s_%s_%s", t.From, t.Symbol, t.To)
		edgeMark[key] = true
	}

	// transition-edges
	for from, trans := range d.Transitions {
		for symbol, to := range trans {
			key := fmt.Sprintf("%s_%s_%s", from, symbol, to)
			if edgeMark[key] {
				fmt.Fprintf(file, `  %s -> %s [label=%s, color=red, penwidth=4];`+"\n", from, to, symbol)
			} else {
				fmt.Fprintf(file, `  %s -> %s [label=%s];`+"\n", from, to, symbol)
			}
		}
	}
	fmt.Fprintf(file, "}")
	return nil
}

/*
TODO:
[] 这个 DFA 的状态过程生成 图形可视化（如 Graphviz .dot 文件
[] “合法性检查”和“所有长度 ≤ N 的合法字符串输出”
添加合法性检查（是否所有状态符号转移完备）

枚举所有规则串（长度 ≤ N）：使用 BFS 枚举输入串并判断是否被接受
重构为模块化包（方便后续实验复用）
将 dfa.Match() 改成可输出状态轨迹
*/

文件：./core/dfa/dfa_model.go
内容：
package dfa

type DFA struct {
	Alphabet     []string                     `json:"alphabet"`
	States       []string                     `json:"states"`
	StartState   string                       `json:"start_state"`
	AcceptStates []string                     `json:"accept_states"`
	Transitions  map[string]map[string]string `json:"transitions"`

	//运行时变量
	acceptMap map[string]bool
}

type TokenType string

const (
	TokenID    TokenType = "ID"  // Identifier
	TokenNUM   TokenType = "NUM" // Number
	TokenFLO   TokenType = "FLO"
	TokenOP    TokenType = "OP"
	TokenDELIM TokenType = "DELIM"
	TokenKW    TokenType = "KEYWORD"
	TokenERROR TokenType = "ERROR"
	TokenWithespace TokenType = "WHITESPACE"
)

type Token struct {
	Type   TokenType
	Lexeme string
}


type DFAWithTokenType struct{
	TokenType TokenType `json:"token_type"`
	DFA *DFA `json:"dfa"`
}

type TransitionTrace struct {
	From   string
	Symbol string
	To     string
}
文件：./core/dfa/load_dfa.go
内容：
package dfa

import (
	"encoding/json"
	"fmt"
	"os"
)

func LoadDFAFromJson(fileName string, verbose bool) (*DFA, error) {
	data, err := os.ReadFile(fileName)
	if err != nil {
		return nil, err
	}
	var dfa DFA
	err = json.Unmarshal(data, &dfa)
	if err != nil {
		return nil, err
	}
	dfa.CheckValidity(verbose, "")
	dfa.buildAcceptMap()
	dfa.ExportDFAtoDot("./dot/dfa.dot")
	return &dfa, nil
}

func LoadMultiDFAFromJson(fileName string, dotPath string, verbose bool) (*[]DFAWithTokenType, error) {
	data, err := os.ReadFile(fileName)
	if err != nil {
		return nil, err
	}
	var dfas []DFAWithTokenType
	err = json.Unmarshal(data, &dfas)
	if err != nil {
		return nil, err
	}
	for i := range dfas {
		if verbose {
			fmt.Printf("Loaded DFA from %s, q0 transitions: %+v\n", fileName, dfas[i].DFA.Transitions["q0"])
		}
		dfas[i].DFA.CheckValidity(verbose, dfas[i].TokenType)
		dfas[i].DFA.buildAcceptMap()
		dotPath := dotPath + "/" + string(dfas[i].TokenType) + ".dot"
		dfas[i].DFA.ExportDFAtoDot(dotPath)
	}
	return &dfas, nil
}

文件：./core/dfa_test.go
内容：
package core_test

import (
	"fmt"
	"lab2/core/dfa"
	"os"
	"testing"
)

func TestDFA(t *testing.T) {
	dfaWithTokenType, err := dfa.LoadMultiDFAFromJson("../json/all_dfa.json", "./dot/test", true)
	if err != nil {
		fmt.Println("Error loading DFA:", err)
		os.Exit(1)
	}

	for _, entry := range *dfaWithTokenType {
		fmt.Printf("\nTesting DFA for token type: %s\n", entry.TokenType)
		fmt.Printf("Alphabet: %v\n", entry.DFA.Alphabet)

		// 测试单字符匹配
		for _, symbol := range entry.DFA.Alphabet {
			ok, _ := entry.DFA.MatchDFA(symbol, true)
			fmt.Printf("Single symbol '%s': matched=%v\n", symbol, ok)
		}

		// 测试复杂词素，针对不同token类型
		testWords := []string{}
		switch entry.TokenType {
		case dfa.TokenID:
			testWords = []string{"int", "x", "var1", "_temp"}
		case dfa.TokenNUM:
			testWords = []string{"0", "123", "4567"}
		case dfa.TokenFLO:
			testWords = []string{"3.1e5", "0.123", ".5", "1e10", "6.022E23"}
		case dfa.TokenOP:
			testWords = []string{"=", "==", "+", "+=", "!"}
		case dfa.TokenDELIM:
			testWords = []string{"(", ")", "{", "}", ";", ","}
		}

		for _, w := range testWords {
			ok, trace := entry.DFA.MatchDFA(w, true)
			fmt.Printf("Word '%s': matched=%v\n", w, ok)
			for _, step := range trace {
				fmt.Printf("  %s --%s--> %s\n", step.From, step.Symbol, step.To)
			}
		}
	}
}

子文件夹：./core/scanner
文件：./core/scanner/scanner.go
内容：
package scanner

import (
	"fmt"
	"lab2/core/dfa"
	"os"
	"unicode"
)

func NewScanner() *Scanner {
	return &Scanner{}
}

func (s *Scanner) RegisterDFA(d *dfa.DFA, t dfa.TokenType) {
	s.DFAList = append(s.DFAList, struct {
		DFA       *dfa.DFA
		TokenType dfa.TokenType
	}{DFA: d, TokenType: t})
}

func isWhitespace(r rune) bool {
	return unicode.IsSpace(r)
}

func (s *Scanner) Scan(input string) (matched dfa.Token, length int) {
	if len(input) == 0 {
		return dfa.Token{Type: dfa.TokenERROR, Lexeme: ""}, 0
	}

	runes := []rune(input)
	if unicode.IsSpace(runes[0]) {
		i := 1
		for i < len(runes) && unicode.IsSpace(runes[i]) {
			i++
		}
		return dfa.Token{Type: dfa.TokenWithespace, Lexeme: string(runes[:i])}, i
	}

	maxLen := 0
	var maxToken dfa.Token
	var maxDFA *dfa.DFA

	for _, entry := range s.DFAList {
		for i := 1; i <= len(runes); i++ {
			sub := string(runes[:i])
			ok, _ := entry.DFA.MatchDFA(sub, false)
			if ok && i > maxLen {
				maxLen = i
				maxToken = dfa.Token{Type: entry.TokenType, Lexeme: sub}
				maxDFA = entry.DFA
			}
		}
	}

	if maxLen == 0 {
		return dfa.Token{Type: dfa.TokenERROR, Lexeme: string(runes[0])}, 1
	}

	if maxToken.Type == dfa.TokenID {
		if keywordType, ok := keywords[maxToken.Lexeme]; ok {
			maxToken.Type = keywordType
		}
	}

	_, _ = maxDFA.MatchDFA(maxToken.Lexeme, true)
	return maxToken, maxLen
}

func ScanAndOutput(scanner *Scanner, input string, dotPath string, out *os.File) {
	pos := 0
	inputRunes := []rune(input)
	length := len(inputRunes)

	for pos < length {
		fmt.Printf("[DEBUG] pos=%d, next char='%c'\n", pos, inputRunes[pos])

		subInput := string(inputRunes[pos:])
		token, tokenLen := scanner.Scan(subInput)
		fmt.Printf("[DEBUG] token='%s', length=%d\n", token.Lexeme, tokenLen)

		if tokenLen == 0 {
			pos++ // 防止死循环
			continue
		}

		if token.Type == dfa.TokenWithespace {
			pos += tokenLen
			fmt.Printf("[main] Skip %d whitespace characters\n", tokenLen)
			continue
		}

		if token.Type == dfa.TokenERROR {
			fmt.Printf("❌ Error: invalid token '%s' at position %d\n", token.Lexeme, pos)
			pos += tokenLen
			continue
		}

		fmt.Fprintf(out, "[Token]: <%s>, [Lexeme]: '%s'\n", token.Type, token.Lexeme)
		fmt.Printf("[Token]: <%s>, [Lexeme]: '%s'\n", token.Type, token.Lexeme)

		_, trace := scanner.DFAList[0].DFA.MatchDFA(token.Lexeme, false)
		dotName := fmt.Sprintf("%s/%s_%d.dot", dotPath, token.Lexeme, pos)
		err := scanner.DFAList[0].DFA.ExportToDot(dotName, trace)
		if err != nil {
			fmt.Println("Export dot failed:", err)
		}

		pos += tokenLen
	}
}

文件：./core/scanner/scanner_model.go
内容：
package scanner

import (
	"lab2/core/dfa"
)

var keywords = map[string]dfa.TokenType{
	"int":      dfa.TokenKW,
	"return":   dfa.TokenKW,
	"if":       dfa.TokenKW,
	"else":     dfa.TokenKW,
	"for":      dfa.TokenKW,
	"while":    dfa.TokenKW,
	"break":    dfa.TokenKW,
	"continue": dfa.TokenKW,
	"void":     dfa.TokenKW,
	"char":     dfa.TokenKW,
	"float":    dfa.TokenKW,
	"double":   dfa.TokenKW,
	// 按需继续添加其他C语言关键字
}

type Scanner struct {
	DFAList []struct {
		DFA       *dfa.DFA
		TokenType dfa.TokenType
	}
}

文件：./core/scanner/scanner_test.go
内容：
package scanner_test

import (
	"lab2/core/dfa"
	"lab2/core/scanner"
	"testing"
)

func TestScanner(t *testing.T) {
	// 加载所有DFA并注册
	dfaWithTokenType, err := dfa.LoadMultiDFAFromJson("../json/all_dfa.json", "./dot/test", true)
	if err != nil {
		t.Fatalf("Failed to load DFA: %v", err)
	}

	scanner := scanner.NewScanner()
	for i := range *dfaWithTokenType {
		scanner.RegisterDFA((*dfaWithTokenType)[i].DFA, (*dfaWithTokenType)[i].TokenType)
	}

	tests := []struct {
		input    string
		expected []dfa.Token
	}{
		{
			input: "int x = 3.15;",
			expected: []dfa.Token{
				{Type: dfa.TokenID, Lexeme: "int"},
				{Type: dfa.TokenID, Lexeme: "x"},
				{Type: dfa.TokenOP, Lexeme: "="},
				{Type: dfa.TokenFLO, Lexeme: "3.15"},
				{Type: dfa.TokenDELIM, Lexeme: ";"},
			},
		},
		{
			input: "var1+=42",
			expected: []dfa.Token{
				{Type: dfa.TokenID, Lexeme: "var1"},
				{Type: dfa.TokenOP, Lexeme: "+="},
				{Type: dfa.TokenNUM, Lexeme: "42"},
			},
		},
		{
			input: "\t \n  \r",
			expected: []dfa.Token{
				{Type: dfa.TokenWithespace, Lexeme: "\t \n  \r"},
			},
		},
		{
			input: "@unknown",
			expected: []dfa.Token{
				{Type: dfa.TokenERROR, Lexeme: "@"},
			},
		},
	}

	for _, tt := range tests {
		pos := 0
		inputRunes := []rune(tt.input)
		var gotTokens []dfa.Token

		for pos < len(inputRunes) {
			token, length := scanner.Scan(string(inputRunes[pos:]))
			if length == 0 {
				t.Fatalf("Zero length token detected, possible infinite loop for input: %s", tt.input)
			}
			pos += length
			if token.Type == dfa.TokenWithespace {
				continue // 忽略空白token
			}
			gotTokens = append(gotTokens, token)
		}

		if len(gotTokens) != len(tt.expected) {
			t.Errorf("Input %q: expected %d tokens, got %d", tt.input, len(tt.expected), len(gotTokens))
			continue
		}
		for i := range gotTokens {
			if gotTokens[i].Type != tt.expected[i].Type || gotTokens[i].Lexeme != tt.expected[i].Lexeme {
				t.Errorf("Input %q: token %d expected (%s, %q), got (%s, %q)",
					tt.input, i, tt.expected[i].Type, tt.expected[i].Lexeme, gotTokens[i].Type, gotTokens[i].Lexeme)
			}
		}
	}
}

子文件夹：./core/utils
文件：./core/utils/utils.go
内容：
package utils


// util function to check if a string exists in a slice
func Contains(slice []string, item string) bool {
	for _, s := range slice {
		if s == item {
			return true
		}
	}
	return false
}

子文件夹：./dot
文件：./go.mod
内容：
module lab2

go 1.23.2

子文件夹：./json
文件：./json/all_dfa.json
内容：
[
    {
    "token_type": "ID",
    "dfa":{
            "alphabet": [
              "a","b","c","d","e","f","g","h","i","j","k","l","m",
              "n","o","p","q","r","s","t","u","v","w","x","y","z",
              "A","B","C","D","E","F","G","H","I","J","K","L","M",
              "N","O","P","Q","R","S","T","U","V","W","X","Y","Z",
              "_","0","1","2","3","4","5","6","7","8","9"
            ],
            "states": ["q0", "q1"],
            "start_state": "q0",
            "accept_states": ["q1"],
            "transitions": {
              "q0": {
                "a": "q1", "b": "q1", "c": "q1", "d": "q1", "e": "q1", "f": "q1", "g": "q1",
                "h": "q1", "i": "q1", "j": "q1", "k": "q1", "l": "q1", "m": "q1", "n": "q1",
                "o": "q1", "p": "q1", "q": "q1", "r": "q1", "s": "q1", "t": "q1", "u": "q1",
                "v": "q1", "w": "q1", "x": "q1", "y": "q1", "z": "q1",
                "A": "q1", "B": "q1", "C": "q1", "D": "q1", "E": "q1", "F": "q1", "G": "q1",
                "H": "q1", "I": "q1", "J": "q1", "K": "q1", "L": "q1", "M": "q1", "N": "q1",
                "O": "q1", "P": "q1", "Q": "q1", "R": "q1", "S": "q1", "T": "q1", "U": "q1",
                "V": "q1", "W": "q1", "X": "q1", "Y": "q1", "Z": "q1",
                "_": "q1"
                    },
              "q1": {
                "a": "q1", "b": "q1", "c": "q1", "d": "q1", "e": "q1", "f": "q1", "g": "q1",
                "h": "q1", "i": "q1", "j": "q1", "k": "q1", "l": "q1", "m": "q1", "n": "q1",
                "o": "q1", "p": "q1", "q": "q1", "r": "q1", "s": "q1", "t": "q1", "u": "q1",
                "v": "q1", "w": "q1", "x": "q1", "y": "q1", "z": "q1",
                "A": "q1", "B": "q1", "C": "q1", "D": "q1", "E": "q1", "F": "q1", "G": "q1",
                "H": "q1", "I": "q1", "J": "q1", "K": "q1", "L": "q1", "M": "q1", "N": "q1",
                "O": "q1", "P": "q1", "Q": "q1", "R": "q1", "S": "q1", "T": "q1", "U": "q1",
                "V": "q1", "W": "q1", "X": "q1", "Y": "q1", "Z": "q1",
                "_": "q1",
                "0": "q1", "1": "q1", "2": "q1", "3": "q1", "4": "q1",
                "5": "q1", "6": "q1", "7": "q1", "8": "q1", "9": "q1"
                    }
                }
            }
    },
    {
        "token_type":"DELIM",
        "dfa":{
            "alphabet": ["(", ")", "{", "}", ";", ","],
            "states": ["q0", "q1"],
            "start_state": "q0",
            "accept_states": ["q1"],
            "transitions": {
            "q0": {
                "(": "q1", ")": "q1", "{": "q1", "}": "q1", ";": "q1", ",": "q1"
                }
            }
            }
    },
    {
        "token_type":"OP",
        "dfa":{
            "alphabet": ["+", "-", "*", "/", "=", "<", ">", "!"],
            "states": ["q0", "q1", "q2"],
            "start_state": "q0",
            "accept_states": ["q1", "q2"],
            "transitions": {
                "q0": {
                   "+": "q1", "-": "q1", "*": "q1", "/": "q1",
                   "=": "q1", "<": "q1", ">": "q1", "!": "q1"
                },
                "q1": {
                  "=": "q2"  
                }
              }
            }

    },
    {
        "token_type":"NUM",
        "dfa":{
            "alphabet": [
                "0", "1", "2", "3", "4", "5", "6", "7", "8", "9"
            ],
            "states": ["q0", "q1"],
            "start_state": "q0",
            "accept_states": ["q1"],
            "transitions": {
                "q0": {
                    "0": "q1", "1": "q1", "2": "q1", "3": "q1", "4": "q1",
                    "5": "q1", "6": "q1", "7": "q1", "8": "q1", "9": "q1"
                },
                "q1": {
                    "0": "q1", "1": "q1", "2": "q1", "3": "q1", "4": "q1",
                    "5": "q1", "6": "q1", "7": "q1", "8": "q1", "9": "q1"
                }
            }
        }
    },
    {
        "token_type":"FLO",
        "dfa":
{
  "alphabet": ["0","1","2","3","4","5","6","7","8","9",".","e","E","+","-"],
  "states": ["q0", "q1", "q2", "q3", "q4", "q5", "q6"],
  "start_state": "q0",
  "accept_states": ["q1","q3", "q6"],
  "transitions": {
    "q0": {
      "0": "q1", "1": "q1", "2": "q1", "3": "q1", "4": "q1",
      "5": "q1", "6": "q1", "7": "q1", "8": "q1", "9": "q1",
      ".": "q2"
    },
    "q1": {
      "0": "q1", "1": "q1", "2": "q1", "3": "q1", "4": "q1",
      "5": "q1", "6": "q1", "7": "q1", "8": "q1", "9": "q1",
      ".": "q2",
      "e": "q4", "E": "q4"
    },
    "q2": {
      "0": "q3", "1": "q3", "2": "q3", "3": "q3", "4": "q3",
      "5": "q3", "6": "q3", "7": "q3", "8": "q3", "9": "q3"
    },
    "q3": {
      "0": "q3", "1": "q3", "2": "q3", "3": "q3", "4": "q3",
      "5": "q3", "6": "q3", "7": "q3", "8": "q3", "9": "q3",
      "e": "q4", "E": "q4"
    },
    "q4": {
      "0": "q6", "1": "q6", "2": "q6", "3": "q6", "4": "q6",
      "5": "q6", "6": "q6", "7": "q6", "8": "q6", "9": "q6",
      "+": "q5", "-": "q5"
    },
    "q5": {
      "0": "q6", "1": "q6", "2": "q6", "3": "q6", "4": "q6",
      "5": "q6", "6": "q6", "7": "q6", "8": "q6", "9": "q6"
    },
    "q6": {
      "0": "q6", "1": "q6", "2": "q6", "3": "q6", "4": "q6",
      "5": "q6", "6": "q6", "7": "q6", "8": "q6", "9": "q6"
    }
  }
}



    }
]
文件：./json/delim.json
内容：
{
  "alphabet": ["(", ")", "{", "}", ";", ","],
  "states": ["q0", "q1"],
  "start_state": "q0",
  "accept_states": ["q1"],
  "transitions": {
    "q0": {
      "(": "q1", ")": "q1", "{": "q1", "}": "q1", ";": "q1", ",": "q1"
    }
  }
}

文件：./json/flo.json
内容：
{
  "alphabet": ["0","1","2","3","4","5","6","7","8","9",".","e","E","+","-"],
  "states": ["q0", "q1", "q2", "q3", "q4", "q5", "q6"],
  "start_state": "q0",
  "accept_states": ["q3", "q6"],
  "transitions": {
    "q0": {
      "0": "q1", "1": "q1", "2": "q1", "3": "q1", "4": "q1",
      "5": "q1", "6": "q1", "7": "q1", "8": "q1", "9": "q1"
    },
    "q1": {
      "0": "q1", "1": "q1", "2": "q1", "3": "q1", "4": "q1",
      "5": "q1", "6": "q1", "7": "q1", "8": "q1", "9": "q1",
      ".": "q2", "e": "q4", "E": "q4"
    },
    "q2": {
      "0": "q3", "1": "q3", "2": "q3", "3": "q3", "4": "q3",
      "5": "q3", "6": "q3", "7": "q3", "8": "q3", "9": "q3"
    },
    "q3": {
      "0": "q3", "1": "q3", "2": "q3", "3": "q3", "4": "q3",
      "5": "q3", "6": "q3", "7": "q3", "8": "q3", "9": "q3",
      "e": "q4", "E": "q4"
    },
    "q4": {
      "0": "q6", "1": "q6", "2": "q6", "3": "q6", "4": "q6",
      "5": "q6", "6": "q6", "7": "q6", "8": "q6", "9": "q6",
      "+": "q5", "-": "q5"
    },
    "q5": {
      "0": "q6", "1": "q6", "2": "q6", "3": "q6", "4": "q6",
      "5": "q6", "6": "q6", "7": "q6", "8": "q6", "9": "q6"
    },
    "q6": {
      "0": "q6", "1": "q6", "2": "q6", "3": "q6", "4": "q6",
      "5": "q6", "6": "q6", "7": "q6", "8": "q6", "9": "q6"
    }
  }
}

文件：./json/id.json
内容：
{
  "alphabet": [
    "a","b","c","d","e","f","g","h","i","j","k","l","m",
    "n","o","p","q","r","s","t","u","v","w","x","y","z",
    "A","B","C","D","E","F","G","H","I","J","K","L","M",
    "N","O","P","Q","R","S","T","U","V","W","X","Y","Z",
    "_","0","1","2","3","4","5","6","7","8","9"
  ],
  "states": ["q0", "q1"],
  "start_state": "q0",
  "accept_states": ["q1"],
  "transitions": {
    "q0": {
      "a": "q1", "b": "q1", "c": "q1", "d": "q1", "e": "q1", "f": "q1", "g": "q1",
      "h": "q1", "i": "q1", "j": "q1", "k": "q1", "l": "q1", "m": "q1", "n": "q1",
      "o": "q1", "p": "q1", "q": "q1", "r": "q1", "s": "q1", "t": "q1", "u": "q1",
      "v": "q1", "w": "q1", "x": "q1", "y": "q1", "z": "q1",
      "A": "q1", "B": "q1", "C": "q1", "D": "q1", "E": "q1", "F": "q1", "G": "q1",
      "H": "q1", "I": "q1", "J": "q1", "K": "q1", "L": "q1", "M": "q1", "N": "q1",
      "O": "q1", "P": "q1", "Q": "q1", "R": "q1", "S": "q1", "T": "q1", "U": "q1",
      "V": "q1", "W": "q1", "X": "q1", "Y": "q1", "Z": "q1",
      "_": "q1"
    },
    "q1": {
      "a": "q1", "b": "q1", "c": "q1", "d": "q1", "e": "q1", "f": "q1", "g": "q1",
      "h": "q1", "i": "q1", "j": "q1", "k": "q1", "l": "q1", "m": "q1", "n": "q1",
      "o": "q1", "p": "q1", "q": "q1", "r": "q1", "s": "q1", "t": "q1", "u": "q1",
      "v": "q1", "w": "q1", "x": "q1", "y": "q1", "z": "q1",
      "A": "q1", "B": "q1", "C": "q1", "D": "q1", "E": "q1", "F": "q1", "G": "q1",
      "H": "q1", "I": "q1", "J": "q1", "K": "q1", "L": "q1", "M": "q1", "N": "q1",
      "O": "q1", "P": "q1", "Q": "q1", "R": "q1", "S": "q1", "T": "q1", "U": "q1",
      "V": "q1", "W": "q1", "X": "q1", "Y": "q1", "Z": "q1",
      "_": "q1",
      "0": "q1", "1": "q1", "2": "q1", "3": "q1", "4": "q1",
      "5": "q1", "6": "q1", "7": "q1", "8": "q1", "9": "q1"
    }
  }
}

文件：./json/num.json
内容：
{
    "alphabet": [
        "0", "1", "2", "3", "4", "5", "6", "7", "8", "9"
    ],
    "states": ["q0", "q1"],
    "start_state": "q0",
    "accept_states": ["q1"],
    "transitions": {
        "q0": {
            "0": "q1", "1": "q1", "2": "q1", "3": "q1", "4": "q1",
            "5": "q1", "6": "q1", "7": "q1", "8": "q1", "9": "q1"
        },
        "q1": {
            "0": "q1", "1": "q1", "2": "q1", "3": "q1", "4": "q1",
            "5": "q1", "6": "q1", "7": "q1", "8": "q1", "9": "q1"
        }
    }
}
文件：./json/op.json
内容：
{
  "alphabet": ["+", "-", "*", "/", "=", "<", ">", "!"],
  "states": ["q0", "q1", "q2"],
  "start_state": "q0",
  "accept_states": ["q1", "q2"],
  "transitions": {
    "q0": {
      "+": "q1", "-": "q1", "*": "q1", "/": "q1",
      "=": "q1", "<": "q1", ">": "q1", "!": "q1"
    },
    "q1": {
      "=": "q2"  
    }
  }
}

文件：./json/test_dfa.json
内容：
{
    "alphabet": [
        "a",
        "b"
    ],
    "states": [
        "q0",
        "q1",
        "q2"
    ],
    "start_state": "q0",
    "accept_states": [
        "q2"
    ],
    "transitions": {
        "q0": {"a": "q1","b": "q0" },
        "q1": { "a": "q2","b": "q0"},
        "q2": {"a": "q2","b": "q2"}
    }
}
文件：./main.go
内容：
package main

import (
	"bufio"
	"flag"
	"fmt"
	"lab2/core/dfa"
	"lab2/core/scanner"
	"os"
	_ "os/exec"
	"strings"
)

func main() {
	// define command line arguments
	dotPath := flag.String("d", "./dot", "Path to save dot file")
	verbose := flag.Bool("v", false, "Verbose output")
	outputFile := flag.String("o", "", "Output file path for tokens (default: print to stdout)")
	sourcefile := flag.String("s", "", "Source file path (default: read from stdin)")
	flag.StringVar(sourcefile, "source", "", "Source file path (default: read from stdin)")
	flag.StringVar(outputFile, "output", "", "Output file path for tokens (default: print to stdout)")
	flag.BoolVar(verbose, "verbose", false, "Verbose output")
	flag.StringVar(dotPath, "dot", "./dot", "Path to save dot file")
	flag.Parse()

	var out *os.File
	var err error
	if *outputFile != "" {
		out, err = os.Create(*outputFile)
		if err != nil {
			fmt.Printf("Failed to create output file: %v\n", err)
			return
		}
		defer out.Close()
	} else {
		out = os.Stdout
	}

	dfaWithTokenType, err := dfa.LoadMultiDFAFromJson("./json/all_dfa.json", *dotPath, *verbose)
	if err != nil {
		fmt.Println("Error loading DFA:", err)
		os.Exit(1)
	}

	newScanner := scanner.NewScanner()
	for i := range *dfaWithTokenType {
		newScanner.RegisterDFA((*dfaWithTokenType)[i].DFA, (*dfaWithTokenType)[i].TokenType)
	}

	if *sourcefile != "" {
		fmt.Printf("parse file: %s\n", *sourcefile)
		contentBytes, err := os.ReadFile(*sourcefile)
		if err != nil {
			fmt.Printf("Failed to read source file: %v\n", err)
			os.Exit(1)
		}
		code := string(contentBytes)
		scanner.ScanAndOutput(newScanner, code, *dotPath, out)
	}

	var reader = bufio.NewReader(os.Stdin)

	for {

		fmt.Print("Enter a string to match (or 'quit' to quit): ")
		line, err := reader.ReadString('\n')
		if err != nil {
			fmt.Println("Error reading input:", err)
			continue
		}
		line = strings.TrimSpace(line)

		if line == "quit" {
			break
		}

		fmt.Fprintf(out, "parsing input: %s\n", line)
		scanner.ScanAndOutput(newScanner, line, *dotPath, out)
		// pos := 0
		// inputRunes := []rune(line)
		// length := len(inputRunes)

		// for pos < length {
		// 	fmt.Printf("[DEBUG] pos=%d, next char='%c'\n", pos, inputRunes[pos])

		// 	subInput := string(inputRunes[pos:])
		// 	token, tokenLen := newScanner.Scan(subInput)
		// 	fmt.Printf("[DEBUG] token='%s', length=%d\n", token.Lexeme, tokenLen)

		// 	if tokenLen == 0 {
		// 		pos++ // 防止死循环
		// 		continue
		// 	}

		// 	if token.Type == dfa.TokenWithespace {
		// 		pos += tokenLen
		// 		fmt.Printf("[main] Skip %d whitespace characters\n", tokenLen)
		// 		continue
		// 	}

		// 	if token.Type == dfa.TokenERROR {
		// 		fmt.Printf("❌ Error: invalid token '%s' at position %d\n", token.Lexeme, pos)
		// 		pos += tokenLen
		// 		continue
		// 	}
		// 	fmt.Fprintf(out, "[Token]: <%s>, [Lexeme]: '%s'\n", token.Type, token.Lexeme)
		// 	fmt.Printf("[Token]: <%s>, [Lexeme]: '%s'\n", token.Type, token.Lexeme)
		// 	_, trace := newScanner.DFAList[0].DFA.MatchDFA(token.Lexeme, false)
		// 	dotName := fmt.Sprintf("%s/%s_%d.dot", *dotPath, token.Lexeme, pos)
		// 	err := newScanner.DFAList[0].DFA.ExportToDot(dotName, trace)
		// 	if err != nil {
		// 		fmt.Println("Export dot failed:", err)
		// 	}

		// 	pos += tokenLen
		// }

	}
}

文件：./output.txt
内容：

文件：./README.md
内容：

- [x] 添加合法性检查（是否所有状态符号转移完备）

- [x] 枚举所有规则串（长度 ≤ N）：使用 BFS 枚举输入串并判断是否被接受

- [x] 将 dfa.Match() 改成可输出状态轨迹

- [x] dot可视化DFA以及状态转移过程

| 功能点                     | 状态     | 备注                                                 |
| ----------------------- | ------ | -------------------------------------------------- |
| 设计多个DFA定义识别各类token      | 已完成    | all\_dfa.json设计了ID, NUM, FLO, OP, DELIM多种token的DFA |
| 加载多个DFA并进行合法性检查         | 已完成    | 加载DFA时进行合法性检查，打印缺失转移等警告                            |
| DFA运行匹配与输出状态转移轨迹        | 已完成    | `MatchDFA`支持verbose，能打印转移步骤                        |
| DFA的DOT文件生成和路径高亮        | 已完成    | 识别路径生成DOT文件，并高亮匹配路径                                |
| 实现Scanner结合所有DFA做最长匹配扫描 | 已完成    | Scanner的Scan支持最长匹配，能区分空白、错误token                   |
| Scanner能对输入字符串进行完整词法分析  | 已完成    | `main.go`实现交互输入并调用Scanner，输出token序列                |
| 单元测试对DFA和Scanner做了基本测试  | 已完成    | 代码中包含DFA测试和Scanner测试，覆盖基本token匹配和扫描测试              |
| 支持空白符和错误token的识别        | 已完成    | 空白符跳过，错误token报告                                    |
| 支持复杂token（如浮点数、复合运算符）   | 已完成    | 浮点数DFA覆盖科学计数法等，运算符支持`==`, `+=`等                    |
| 支持输出扫描结果到终端或文件          | 代码框架完成 | 目前main.go终端交互输出完整，文件重定向可通过shell实现                  |
| flex正则式实现（选做）           | 未见相关代码 | 未提供flex正则表达式实现                                     |

